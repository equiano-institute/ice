{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0427d40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 15\n",
      "5 / 15\n",
      "10 / 15\n",
      "15 / 15\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import json\n",
    "from fvalues import F\n",
    "from ice.recipe import recipe\n",
    "import pandas as pd\n",
    "from ice.recipes.primer.subquestions import ask_subquestions\n",
    "from ice.utils import map_async\n",
    "from amplification import answer_by_amplification\n",
    "from prompt_generator import make_scenario_prompt, make_evaluation_prompt\n",
    "\n",
    "Question = str\n",
    "Answer = str\n",
    "Subs = list[tuple[Question, Answer]]\n",
    "\n",
    "\n",
    "def read_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def render_background(subs: Subs) -> str:\n",
    "    subs_text = \"\\n\\n\".join(f\"Q: {q} A: {a}\" for (q, a) in subs)\n",
    "    return f\"Here is relevant background information \\n\\n{subs_text}\\n\\n\"\n",
    "\n",
    "\n",
    "\n",
    "class RecipeModel():\n",
    "    def __init__(self, agent_name):\n",
    "        self.agent_name = agent_name\n",
    "\n",
    "    def generate_text(self, inputs, max_length=250):\n",
    "        response = recipe.agent(agent_name=self.agent_name).complete(prompt=inputs, stop='\"', max_tokens=max_length)\n",
    "        return response\n",
    "\n",
    "async def get_standard_answer(question: str, agent_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using the standard approach\n",
    "    \"\"\"\n",
    "    model = RecipeModel(agent_name=agent_name)\n",
    "    prompt = make_scenario_prompt(question)\n",
    "    answer = await model.generate_text(prompt)\n",
    "    return answer\n",
    "\n",
    "async def get_factored_answer(question: str, agent_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate an answer using the factored cognition approach\n",
    "    \"\"\"\n",
    "    prompt = make_scenario_prompt(question)\n",
    "    factored_answer, subs = await answer_by_amplification(prompt, engine=agent_name)\n",
    "    return factored_answer, subs\n",
    "\n",
    "async def evaluate_tasks(task_name, agent_name, task_count):\n",
    "    task_path = os.path.join('/Users/bensturgeon/werk/ice/benchmarks/', task_name)\n",
    "    task_data = read_jsonl(task_path)\n",
    "\n",
    "    total_questions = 0\n",
    "    evaluations = []\n",
    "    result = {}\n",
    "\n",
    "    \n",
    "    result[\"count_factored\"] = 0\n",
    "    result[\"count_baseline\"] = 0\n",
    "    result[\"count_total\"] = 0\n",
    "    result[\"count_undecided\"] = 0\n",
    "\n",
    "\n",
    "    for index, row in enumerate(task_data):\n",
    "        if index %5 == 0:\n",
    "            print(f\"{index} / {task_count}\")\n",
    "        scenario = row['text']\n",
    "\n",
    "        # Generate answers using both approaches\n",
    "        standard_answer = await get_standard_answer(scenario, agent_name)\n",
    "        factored_answer, subs = await get_factored_answer(scenario, agent_name)\n",
    "\n",
    "        # Evaluate the answers\n",
    "        evaluation_model = RecipeModel(agent_name=\"gpt-4\")\n",
    "        evaluation_prompt= make_evaluation_prompt(scenario, standard_answer, factored_answer)\n",
    "        evaluation_result = await evaluation_model.generate_text(evaluation_prompt)\n",
    "        \n",
    "        if evaluation_result == \"2\":\n",
    "            result[\"count_factored\"] +=1\n",
    "        elif evaluation_result == \"1\":\n",
    "            result[\"count_baseline\"] +=1\n",
    "        elif evaluation_result == \"0\":\n",
    "            result[\"count_undecided\"] +=1\n",
    "        else:\n",
    "            \n",
    "            print(\"Invalid output from evaluator.\")\n",
    "        # Store the results\n",
    "        evaluations.append({\n",
    "            'sample_number': index+1,\n",
    "            'input': scenario,\n",
    "            'standard_answer': standard_answer,\n",
    "            'factored_answer': factored_answer,\n",
    "            'trace': subs,\n",
    "            'evaluation_result': evaluation_result\n",
    "        })\n",
    "\n",
    "        total_questions += 1\n",
    "\n",
    "\n",
    "\n",
    "        result['evaluations'] = evaluations\n",
    "\n",
    "        result[\"factored_percentage\"] = result[\"count_factored\"]/(index +1)\n",
    "        result[\"baseline_percentage\"] = result[\"count_baseline\"]/(index +1)\n",
    "\n",
    "\n",
    "\n",
    "        # Save the results to a file\n",
    "        task_name_no_extension = task_name.split(\".csv\")[0]\n",
    "        with open(f'benchmarks/results/{task_name_no_extension}_{agent_name}_evaluations.json', 'w') as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "\n",
    "        if index >= task_count:\n",
    "            break\n",
    "    \n",
    "\n",
    "    del evaluations\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent_name = \"chatgpt\"\n",
    "    model = RecipeModel(agent_name=agent_name)\n",
    "    # accuracy, answers = await evaluate_task('ethics_suite.jsonl', model, agent_name, 300)\n",
    "    await evaluate_tasks(\"ethics_suite.jsonl\",agent_name, 15)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b71e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answer = await model.generate_text(\"What is the sky?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
